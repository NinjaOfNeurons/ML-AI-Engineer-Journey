# Machine Learning â€“ Important Notes (Concise)

> ğŸ“Œ **Focused, exam + interview + revision ready notes**
> Only **important concepts** kept, noise removed.

---

## 1. Learning Types

### Supervised Learning

* Learns from **labeled data (x, y)**
* Learns by comparing predictions with **right answers**
* Examples:

  * Linear Regression
  * Logistic Regression

### Unsupervised Learning

* Data has **inputs x only**, no labels y
* Goal â†’ **find structure in data**

#### Types of Unsupervised Learning

* **Clustering** â†’ group similar data points
* **Anomaly Detection** â†’ find unusual / rare points
* **Dimensionality Reduction** â†’ compress data

---

## 2. Linear Regression (Core Intuition)

### Important Note âš ï¸

**Linear â‰  straight line**

Linear means:

* Linear **with respect to parameters (w, b)**

Model:

```
f(x) = wx + b
```

* **w** â†’ slope / weight
* **b** â†’ bias / shift

By tuning `w` and `b`, we get the **best-fit line**.

---

## 3. Cost Function

* Cost function = **error measure**
* Tells how bad current parameters are
* Goal â†’ **minimize cost**

---

## 4. Gradient Descent (Very Important â­)

### Core Idea

**Gradient Descent = downhill movement**

* Cost surface = hill
* Height = error
* Minimum = best (w, b)

### Gradient

* Gradient = **uphill direction**
* To go downhill â†’ move in **negative gradient** direction

### Update Rule

```
w = w âˆ’ Î± * âˆ‚J/âˆ‚w
```

* `Î± (alpha)` â†’ learning rate (step size)
* `âˆ‚J/âˆ‚w` â†’ slope of cost function

### Learning Rate

* Too large â†’ overshoot
* Too small â†’ slow convergence

---

## 5. Types of Gradient Descent

### Batch Gradient Descent

* Uses **entire dataset** per update
* âœ… Stable, smooth convergence
* âŒ Slow for large datasets

### Mini-batch Gradient Descent âœ…

* Uses small batches
* Best trade-off between **speed & stability**

---

## 6. Multiple Features (Vector Form)

* Feature vector:

```
x = [xâ‚, xâ‚‚, ..., xâ‚™]
```

* Parameters:

```
w = [wâ‚, wâ‚‚, ..., wâ‚™]
```

* Prediction:

```
f(x) = w Â· x + b
```

### Vectorization â­

* Faster computation
* Cleaner math
* Gradient Descent remains same â†’ **more wâ€™s only**

---

## 7. Normal Equation

* Direct solution (no iterations)
* Computationally expensive
* Not suitable for large datasets

---

## 8. Feature Scaling (Very Important â­â­)

### Problem (Tall & Skinny Contours)

* Features have very different ranges
* Gradient Descent zig-zags â†’ slow convergence

### Solution

**Normalize features**

#### Techniques

* Divide by max value
* Mean normalization
* Z-score normalization

ğŸ‘‰ Result: **fast & stable convergence**

---

## 9. Checking Convergence

### Learning Curve

* Cost should decrease every iteration
* Flat curve â†’ convergence reached

### Automatic Convergence Test

* Stop if cost decrease < Îµ (epsilon)

---

## 10. Feature Engineering (Very Important â­â­)

* Create better features using intuition
* Example:

  * Depth + Width â†’ Area

ğŸ‘‰ Better features = better performance

---

## 11. Polynomial Regression

### Important Clarification âš ï¸

**Polynomial Regression â‰  Non-linear model**

* Still linear in parameters
* Uses polynomial features

---

## ğŸ” Final Revision Snapshot

* Supervised â†’ labels
* Unsupervised â†’ structure
* Linear â‰  straight line
* w = slope, b = shift
* Cost function = error
* Gradient = uphill direction
* Minus sign = downhill move
* Alpha = step size
* Batch GD = stable but slow
* Mini-batch = practical choice
* Vectorization = speed + clarity
* Feature scaling â†’ fast convergence â­
* Feature engineering â†’ performance boost â­
* Polynomial regression â‰  non-linear model â­
